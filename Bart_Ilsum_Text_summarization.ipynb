{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## VIJAY KRISHNA RV\n## ME21B1039","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:01:06.710526Z","iopub.status.idle":"2025-04-11T13:01:06.710810Z","shell.execute_reply.started":"2025-04-11T13:01:06.710664Z","shell.execute_reply":"2025-04-11T13:01:06.710677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --quiet \\\n  transformers==4.38.2 \\\n  datasets==2.19.1 \\\n  evaluate==0.4.1 \\\n  accelerate==0.27.2 \\\n  peft==0.8.2 \\\n  rouge-score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:19:51.395764Z","iopub.execute_input":"2025-04-11T13:19:51.396379Z","iopub.status.idle":"2025-04-11T13:21:16.879839Z","shell.execute_reply.started":"2025-04-11T13:19:51.396354Z","shell.execute_reply":"2025-04-11T13:21:16.878952Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.3.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n)\nimport evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:21:16.881349Z","iopub.execute_input":"2025-04-11T13:21:16.881627Z","iopub.status.idle":"2025-04-11T13:21:40.280309Z","shell.execute_reply.started":"2025-04-11T13:21:16.881599Z","shell.execute_reply":"2025-04-11T13:21:40.279523Z"}},"outputs":[{"name":"stderr","text":"2025-04-11 13:21:25.249949: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744377685.443390      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744377685.503824      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load ILSUM English subset\ndataset = load_dataset(\"ILSUM/ILSUM-1.0\", split=\"train\", name=\"English\")\n\nMAX_INPUT_LENGTH = 512\nMAX_TARGET_LENGTH = 128\n\ndef filter_example(example):\n    return len(example['Article'].split()) <= MAX_INPUT_LENGTH and len(example['Summary'].split()) <= MAX_TARGET_LENGTH\n\ndataset = dataset.filter(filter_example)\n\n# Train/val/test split\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_valid = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n\ntrain_dataset = train_valid[\"train\"]\nval_dataset = train_valid[\"test\"]\ntest_dataset = dataset[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:21:44.556076Z","iopub.execute_input":"2025-04-11T13:21:44.557246Z","iopub.status.idle":"2025-04-11T13:21:49.517086Z","shell.execute_reply.started":"2025-04-11T13:21:44.557218Z","shell.execute_reply":"2025-04-11T13:21:49.516550Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c390ec45e1684a33b06ad8e758b15ed3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/46.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe9a516ba2e4759b9eef6868eb6a7cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/16.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df324550e5b64c44968110d4d1afa3b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"695c3b63b0dd4fd3b7d1e52dab430f4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/12565 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"620b466e81fd42cfa633ec0d0398f857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4487 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca29a8018bb48e7bd1654d9b02288b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/898 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d34e7b2f7e7e4fabae75792cfde57897"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/12565 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0a49ff260145c5b1306cce40c12f09"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n\ndef preprocess_function(example):\n    model_inputs = tokenizer(\n        example[\"Article\"],\n        max_length=MAX_INPUT_LENGTH,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            example[\"Summary\"],\n            max_length=MAX_TARGET_LENGTH,\n            truncation=True,\n            padding=\"max_length\"\n        )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\nval_dataset = val_dataset.map(preprocess_function, batched=True)\ntest_dataset = test_dataset.map(preprocess_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:21:56.882239Z","iopub.execute_input":"2025-04-11T13:21:56.882520Z","iopub.status.idle":"2025-04-11T13:22:04.392755Z","shell.execute_reply.started":"2025-04-11T13:21:56.882499Z","shell.execute_reply":"2025-04-11T13:22:04.392042Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24cb7e75df540c1a29f3039c53a4c07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"090dcca4f0d14465b00b66ba15d0ccee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72935e45891c4f09800d03a53e29f439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af9753a3abf4d2db87d8b0efb988f03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5673 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4b8b81fed24a4ca8785ff33363a279"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/631 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"852d65b3c9d34e74a7c37b21027a3156"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1576 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0cabdc092194be0991b3b31b086c927"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:22:10.926697Z","iopub.execute_input":"2025-04-11T13:22:10.927326Z","iopub.status.idle":"2025-04-11T13:22:13.914010Z","shell.execute_reply.started":"2025-04-11T13:22:10.927301Z","shell.execute_reply":"2025-04-11T13:22:13.913444Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ffd120bc22b4be886129e7a689c9c4d"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=5,           # For testing\n    predict_with_generate=True,\n    logging_dir=\"./logs\",\n    logging_steps=10,             # ⬅️ Log every 10 steps\n    report_to=\"none\"              # ⬅️ No external logging (just print)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:22:41.699217Z","iopub.execute_input":"2025-04-11T13:22:41.699714Z","iopub.status.idle":"2025-04-11T13:22:41.704571Z","shell.execute_reply.started":"2025-04-11T13:22:41.699694Z","shell.execute_reply":"2025-04-11T13:22:41.703855Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\nscorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    rouge1 = rouge2 = rougeL = 0.0\n    for pred, label in zip(decoded_preds, decoded_labels):\n        scores = scorer.score(label, pred)\n        rouge1 += scores[\"rouge1\"].fmeasure\n        rouge2 += scores[\"rouge2\"].fmeasure\n        rougeL += scores[\"rougeL\"].fmeasure\n\n    total = len(decoded_preds)\n    return {\n        \"rouge1\": round(rouge1 / total * 100, 2),\n        \"rouge2\": round(rouge2 / total * 100, 2),\n        \"rougeL\": round(rougeL / total * 100, 2),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:22:51.981490Z","iopub.execute_input":"2025-04-11T13:22:51.982127Z","iopub.status.idle":"2025-04-11T13:22:52.441331Z","shell.execute_reply.started":"2025-04-11T13:22:51.982102Z","shell.execute_reply":"2025-04-11T13:22:52.440575Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:23:03.563457Z","iopub.execute_input":"2025-04-11T13:23:03.563757Z","iopub.status.idle":"2025-04-11T13:23:03.876568Z","shell.execute_reply.started":"2025-04-11T13:23:03.563736Z","shell.execute_reply":"2025-04-11T13:23:03.876000Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:23:05.784697Z","iopub.execute_input":"2025-04-11T13:23:05.784970Z","iopub.status.idle":"2025-04-11T14:05:49.064911Z","shell.execute_reply.started":"2025-04-11T13:23:05.784950Z","shell.execute_reply":"2025-04-11T14:05:49.064256Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7095' max='7095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7095/7095 42:41, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.103300</td>\n      <td>0.103154</td>\n      <td>33.690000</td>\n      <td>23.670000</td>\n      <td>30.950000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.108900</td>\n      <td>0.102206</td>\n      <td>36.590000</td>\n      <td>27.190000</td>\n      <td>33.940000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.066800</td>\n      <td>0.101788</td>\n      <td>36.520000</td>\n      <td>27.410000</td>\n      <td>34.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.089800</td>\n      <td>0.102981</td>\n      <td>36.590000</td>\n      <td>27.660000</td>\n      <td>34.150000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.065500</td>\n      <td>0.103503</td>\n      <td>36.280000</td>\n      <td>27.310000</td>\n      <td>33.880000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=7095, training_loss=0.15773942608115873, metrics={'train_runtime': 2562.8579, 'train_samples_per_second': 11.068, 'train_steps_per_second': 2.768, 'total_flos': 8647587318988800.0, 'train_loss': 0.15773942608115873, 'epoch': 5.0})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n\nsample_article = \"\"\"\nIndia’s Chandrayaan-3 mission successfully landed on the Moon, marking a historic moment for the country’s space program.\nThe lander touched down near the lunar south pole, making India the first nation to achieve this feat.\n\"\"\"\n\nsummary = summarizer(sample_article, max_length=128, min_length=30, do_sample=False)\nprint(\"📝 Generated Summary:\\n\", summary[0][\"summary_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:22:18.499762Z","iopub.execute_input":"2025-04-11T14:22:18.500317Z","iopub.status.idle":"2025-04-11T14:22:20.856299Z","shell.execute_reply.started":"2025-04-11T14:22:18.500295Z","shell.execute_reply":"2025-04-11T14:22:20.855502Z"}},"outputs":[{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n","output_type":"stream"},{"name":"stdout","text":"📝 Generated Summary:\n The lander touched down near the lunar south pole, making India the first nation to achieve this feat. The Indian Space Agency said that the Chandrayaan-3 mission successfully landed on the Moon.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"long_article = \"\"\"\nIndia’s Chandrayaan-3 mission successfully landed on the Moon, marking a historic milestone for the country’s space program. \nThe lander, named Vikram, touched down near the unexplored lunar south pole — a region where no spacecraft has ever landed before. \nThis makes India the first country to reach this part of the Moon. The Indian Space Research Organisation (ISRO) celebrated this achievement, \nhighlighting that it demonstrates India’s growing capabilities in space exploration. The rover onboard, Pragyan, will now begin its mission \nto analyze the lunar surface for signs of water ice and important minerals. This success comes after the Chandrayaan-2 mission failed to land safely in 2019. \nPrime Minister Narendra Modi praised the mission, stating that “India is now on the Moon” and dedicating the moment to scientists and citizens alike.\n\"\"\"\n\nsummary = summarizer(long_article, max_length=100, min_length=40, do_sample=False)\nprint(\"📝 Generated Summary:\\n\", summary[0][\"summary_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:27:03.356313Z","iopub.execute_input":"2025-04-11T14:27:03.356631Z","iopub.status.idle":"2025-04-11T14:27:06.119303Z","shell.execute_reply.started":"2025-04-11T14:27:03.356606Z","shell.execute_reply":"2025-04-11T14:27:06.118569Z"}},"outputs":[{"name":"stdout","text":"📝 Generated Summary:\n This makes India the first country to reach this part of the Moon. The Indian Space Research Organisation (ISRO) celebrated this achievement,  highlighting that it demonstrates India’s growing capabilities in space exploration.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"article_1 = \"\"\"\nIndia’s Chandrayaan-3 mission successfully landed on the Moon, marking a historic milestone for the country’s space program. \nThe lander, named Vikram, touched down near the unexplored lunar south pole — a region where no spacecraft has ever landed before. \nThis makes India the first country to reach this part of the Moon. The Indian Space Research Organisation (ISRO) celebrated this achievement, \nhighlighting that it demonstrates India’s growing capabilities in space exploration. The rover onboard, Pragyan, will now begin its mission \nto analyze the lunar surface for signs of water ice and important minerals. This success comes after the Chandrayaan-2 mission failed to land safely in 2019. \nPrime Minister Narendra Modi praised the mission, stating that “India is now on the Moon” and dedicating the moment to scientists and citizens alike.\n\"\"\"\narticle_2 = \"\"\"\nIn a landmark deal, over 190 countries have agreed on a new global framework to address climate change at the United Nations Climate Conference. \nThe agreement sets ambitious targets for reducing carbon emissions and aims to limit global warming to 1.5 degrees Celsius above pre-industrial levels. \nNations have pledged to move away from fossil fuels, invest in renewable energy, and protect biodiversity. \nThe deal also includes a $100 billion annual fund to help developing countries adapt to climate impacts. \nWhile activists say more aggressive action is needed, world leaders have called the agreement a significant step toward a sustainable future.\n\"\"\"\narticle_3 = \"\"\"\nScientists have announced a major breakthrough in mRNA vaccine technology. A new vaccine has shown remarkable success in protecting against a wide range of flu strains in early human trials. \nUnlike traditional vaccines, the mRNA platform allows for faster development and easier updates to match mutating viruses. \nThis advancement could revolutionize how we respond to seasonal flu and future pandemics. \nExperts believe the new technology will also enhance vaccine equity by enabling lower-cost production and simpler storage requirements. \nThe study, published in the New England Journal of Medicine, is being hailed as a milestone in preventative medicine.\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:30:44.335480Z","iopub.execute_input":"2025-04-11T14:30:44.335768Z","iopub.status.idle":"2025-04-11T14:30:44.342803Z","shell.execute_reply.started":"2025-04-11T14:30:44.335749Z","shell.execute_reply":"2025-04-11T14:30:44.342193Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"articles = [article_1, article_2, article_3]\n\nfor i, article in enumerate(articles, 1):\n    summary = summarizer(article, max_length=100, min_length=40, do_sample=False)\n    print(f\"📰 Article {i} Summary:\\n{summary[0]['summary_text']}\\n{'-'*80}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:30:45.121182Z","iopub.execute_input":"2025-04-11T14:30:45.121423Z","iopub.status.idle":"2025-04-11T14:30:52.923201Z","shell.execute_reply.started":"2025-04-11T14:30:45.121407Z","shell.execute_reply":"2025-04-11T14:30:52.922497Z"}},"outputs":[{"name":"stdout","text":"📰 Article 1 Summary:\nThis makes India the first country to reach this part of the Moon. The Indian Space Research Organisation (ISRO) celebrated this achievement,  highlighting that it demonstrates India’s growing capabilities in space exploration.\n--------------------------------------------------------------------------------\n📰 Article 2 Summary:\nThe agreement sets ambitious targets for reducing carbon emissions and aims to limit global warming to 1.5 degrees Celsius above pre-industrial levels. Nations have pledged to move away from fossil fuels, invest in renewable energy, and protect biodiversity.\n--------------------------------------------------------------------------------\n📰 Article 3 Summary:\nScientists have announced a major breakthrough in mRNA vaccine technology. A new vaccine has shown remarkable success in protecting against a wide range of flu strains in early human trials. The study, published in the New England Journal of Medicine, is being hailed as a milestone in preventative medicine.\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch\n\n# Move model to GPU (if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Evaluate on test set\nmetrics = trainer.evaluate(test_dataset)\nprint(\"📊 Test Set Evaluation Metrics:\")\nfor k, v in metrics.items():\n    print(f\"{k}: {round(v, 4)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:32:32.876964Z","iopub.execute_input":"2025-04-11T14:32:32.877727Z","iopub.status.idle":"2025-04-11T14:36:28.956851Z","shell.execute_reply.started":"2025-04-11T14:32:32.877706Z","shell.execute_reply":"2025-04-11T14:36:28.956034Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='394' max='394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [394/394 03:53]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"📊 Test Set Evaluation Metrics:\neval_loss: 0.1136\neval_rouge1: 50.18\neval_rouge2: 37.73\neval_rougeL: 45.38\neval_runtime: 235.912\neval_samples_per_second: 6.68\neval_steps_per_second: 1.67\nepoch: 5.0\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"model.save_pretrained(\"./bart-summarizer-ilsum\")\ntokenizer.save_pretrained(\"./bart-summarizer-ilsum\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:37:41.380911Z","iopub.execute_input":"2025-04-11T14:37:41.381613Z","iopub.status.idle":"2025-04-11T14:37:42.347941Z","shell.execute_reply.started":"2025-04-11T14:37:41.381586Z","shell.execute_reply":"2025-04-11T14:37:42.347359Z"}},"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 128, 'min_length': 12, 'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('./bart-summarizer-ilsum/tokenizer_config.json',\n './bart-summarizer-ilsum/special_tokens_map.json',\n './bart-summarizer-ilsum/vocab.json',\n './bart-summarizer-ilsum/merges.txt',\n './bart-summarizer-ilsum/added_tokens.json',\n './bart-summarizer-ilsum/tokenizer.json')"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}